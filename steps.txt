Create an environment.
Activate an environment.
Create README.md
connect git and push README.md
create git ignore with pyhon and pull in vscode
Create folder structure and Imp files.
note= .env will store imp credentials and we will access it via python-dotenv library we load credentials via load_dotenv() and can access vis os.getenv()...
Connect poetry (poetry init)
Add dependencies (poetry add)
poetry install for confirmation.
Add logging and exception handling.
Set up MongoDB--> ATLAS account for free access.
Use its code to check if mongo db is connected or not @ test_mongodb.pyhon

ETL_PIPELINE: 
Set up ETL pipeline with python (source(read data) --> Transform --> load) @ etl_pipeline.py
    in this step we are push json data to mongo db...How?
    first create connecrtion via mongodb_client to mongodb
    then in that mongodb we add database
    then in that database we add collection
    then in that collection we add our records via collection.insert_many(records)
        here we transform our records(csv) to list of json 
    check_out @ etl_pipeline.py


DATA_INGESTION Configration:
    check_out data_ingestion.jpg, this is the basic path for taking data from database the store it in feature store/
    then export raw data from feature store and apply EDA and FE then split the data in to train and test and store it for further process.
        Now before working on data_ingestion: we have to set up path of the files for storage @entity/config_entity.py
        For that we have to define common constant variable for training pipeline & data Ingestion related constant @constant/training_pipeline/__init__.py

data_ingestion Component:
    step 1: Read the data from mongodb
    step 2: create the feature store 
    step 3: split the data into train and test
    step 4: store splited file into ingested folder

data_ingestion_artifacts...
we have a train and test data file now we are storing the file path for train & test data which will be used in further processes.
thwn we test it in main...

Data Validation:
    Drift: e.g. initially data follow normal bell curve, after some time it may be skew one side and need to be tune model. 
what we need(schema):
    data validation directory, validation data dr, invalid data dr, valid train file path, invalid train file path, invalid test file path, drift report file path
    First create a constant for schema in training_pipeline __init__.py file (name of the directory)
    Second in config_entity create a file path for all directory.
    Thirt create a file in component --> data_validation.py | Check project structure "Data validation Component".
    Forth create class DataValidationArtifacts in artifact_entity to store all file(output) of data validation component
    To validate num of column, numerical colmn exist or not, Create a schema folder and add file path for schema.yml in training_pipeline init file...
    Then add columns name with its datatype and num column names in propper format. We will use it for validation....
    Create a class DataValidation in data_validation.py :
        First we read the schema file
            we will read the file in utils--> func: read_yaml_file(), generally utils useed for reading, loading, saving file.
        Second  create function initiate_data_validation --> data_validation path
            Read train and test file for validation pourpose..
            create func in clsdd Datavalidation where validation of number of col and initialize here...
            Also validate numerical columns
            For checking data drift create func: detect_dataset_drift --> Datavalidation()
            after checking store train and test csv in Artifacts/data_validation/validated/csv files
            Also create drift report and store it at Artifacts/data_validation/drift_report
            at the end store necessary path at --> data_validation_artifact in entity/artifact_entity.py and return data_validation_artifact(all necessary paths)


Data Transformation Componenet:
    First set all transformation file paths: path from where data is coming and storage_location for transformed data 
        Create how data coming from the transformation will store @utils.py 
    Second Transforming data @data_transformation
    1st create class DataTransformation()
        create func: initiate_data_trasformation
        write whole process inside it and create necessary func in the class such as read_data and get_data_transformer_obj
        return data_transformation_artifacts

Model Trainer & Evaluation Componenet:
    As usual first setup file paths, configration and artifacts for model trainer is setup. @training_pipeline, config_entity.py and artifact_entity.py
    Then create ml_utils in utils folder and create metric and model file: inside metric -> classification_metric.py 
        which contain func: get_classification_score() gives f1, precision, recall socre and store in ClassificationMetricArtifacts which further used to 
        track scores in mlflow and stored in ModelTrainerArtifacts.
    In model file create estimator.py -> create NetworkModel class which will take preprocessor and best model as an attribute and also consist predict method: useful to create an object for the next process: evaluation Componenet.
        will create an object in train_model method of ModelTrainer()
    Create a method: track_mlflow() in ModelTrainer() to track the result of ClassificationMetricArtifacts and storing best model.
    Run initiate_model_trainer and return model_trainer_artifacts.

Model Pusher Component:
    Final Model: Create a final model folder at root We will store preprocessor object and final model here.
    generally we can store final obj or model on cloud such as AWS S3 etc.

How we can run the model training locally and track mlflow remotely: Ans-> via ML Dagshub: DagsHub is a platform for AI and ML developers that lets you manage and collaborate on your data, models, experiments, alongside your code.
Dagshub: through it what we want is when we run code in local and mlflow hits all the logs ot tracking files will be loaded in remote repository not in local...
Note: MLflow: Open-source, focuses on the ML lifecycle (tracking, models, etc.).
      DAGsHub: Open-source, focuses on collaboration, version control (Git/DVC), and tracking.
        Step 1: conncet github repository in dagshub.
        Step 2: connect dagshub repo with local repo u need to install dagshub
        Step 3: Follow steps of dagshub @ experiments
        Step 4: Run the experiment locally all the tracking file and model saved at remote repository in dagshub.
    Note: we can give the url to anybody to track our progress. 

Training Pipeline: It is always better to create a class for running whole processes
    Create training_pipeline.py to run whole processes @pipeline.

APP: 
    Create app.py 
        use Fast Api for front end.
        from fastapi.middleware.cors import CORSMiddleware:
        CORSMiddleware:
        Cross-Origin Resource Sharing: security feature implemented by web browsers to control how resources on a web server can be requested from another domain. 
        By default, browsers block cross-origin requests for security reasons, preventing one website from accessing resources on another. 
        CORS allows servers to specify who can access their resources, what methods are allowed (GET, POST, etc.), and 
        what headers can be included in the request.

Batch Predictions: 
    what if we need prediction for new input features which don't have Result/Output. And we have to predict. We do BP.
    here we use the class NetworkModel from utils->ml_utils->model->NetworkModel for prediction
    create a func: predict_route in app, use upload file and predict based on file data store prediction in Prediction Output folder with input data.

AWS S3 Bucket:
    Now it is better to push or having a replica of final model and artifacts in cloud for that we will use AWS S3 Bucket.
    S3 Bucket: S3 buckets are a reliable and flexible way to store large amounts of unstructured data, with easy access, high durability, and scalable storage options.
        Use Cases:
            Storing static website content, backups, large datasets, and media files.
            Sharing files or hosting files for websites or applications.

    We will push this data final model and artifacts from our training pipeline.
    add its functions in the training pipeline and run

    S3 account set up:
        download aws cli in pc
        first create s3 bucket, setup same name in constant to access this bucket  
        Second create IAM user use its secret credentials to set up aws configure
        Now run the pipeline
        Note: don't share crendential and push it in git...please!!!

github workflow:
    write configration for EC2 deployment..do chat gpt and follow code...

Docker Image:
    dockerfile: create it 
        apt: apt provides a high-level Command Line Interface (CLI) for the APT package management system, 
            offering a user-friendly interface intended for interactive use. It simplifies common tasks like installation, upgrades, 
            and removal, with better defaults than more specialized tools like apt-get and apt-cache.
        apt-get: apt-get is a command-line tool that helps in handling packages in Linux. 
                Its main task is to retrieve the information and packages from the authenticated sources for installation, upgrade, and 
                removal of packages along with their dependencies. Here APT stands for Advanced Packaging Tool
        Build it and run app through it, if all right then we are good to go for deployment
        
ECR REPOSITORY: aws cloud storage
    create it to store docker image
    carefully arrange and store credentials according to github workflow for deployment





    



     


